{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6c1370e-15f5-4092-a19a-dadd698b5a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/shugo/Desktop/SIGNATE/SIGNATE_StudentCup2021/shu421/notebooks', '/Library/Frameworks/Python.framework/Versions/3.7/lib/python37.zip', '/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7', '/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/lib-dynload', '', '/Users/shugo/Desktop/SIGNATE/SIGNATE_StudentCup2021/shu421/lib/python3.7/site-packages', '/Users/shugo/Desktop/SIGNATE/SIGNATE_StudentCup2021/shu421/lib/python3.7/site-packages/IPython/extensions', '/Users/shugo/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import sys\n",
    "print(sys.path)\n",
    "sys.path.append('../modules/')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_origin = pd.read_csv('../input/train.csv')\n",
    "test_origin = pd.read_csv('../input/test.csv')\n",
    "\n",
    "df_train = train_origin.copy()\n",
    "df_test = test_origin.copy()\n",
    "\n",
    "df_sample_sub = pd.read_csv('../input/sample_submit.csv', header=None)\n",
    "df_sample_sub.columns = ['index', 'genre']\n",
    "df_genre_labels = pd.read_csv('../input/genre_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abbeaf0e-ff00-4d93-b854-ce30bdf3918d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = 11\n",
    "\n",
    "# testのジャンルを-100として結合\n",
    "def merge_train_test(df_train, df_test):\n",
    "    if 'genre' not in df_test.columns.tolist():\n",
    "        df_test['genre'] = -100\n",
    "    res = pd.concat([df_train, df_test])\n",
    "    res.reset_index(inplace=True, drop = True)\n",
    "    return res\n",
    "\n",
    "def split_train_test(df):\n",
    "    df_train = df[df['genre'] != -100]\n",
    "    df_test = df[df['genre'] == -100]\n",
    "    df_train.reset_index(inplace=True, drop=True)\n",
    "    df_test.reset_index(inplace=True, drop =True)\n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "\n",
    "warnings.simplefilter('ignore', pd.core.common.SettingWithCopyWarning)\n",
    "warnings.simplefilter('ignore', UserWarning)\n",
    "\n",
    "\n",
    "N_CLASSES = 11\n",
    "\n",
    "\n",
    "INPUT = Path(\"../input\")\n",
    "df_train = pd.read_csv(INPUT / \"train.csv\")\n",
    "df_test = pd.read_csv(INPUT / \"test.csv\")\n",
    "df_sample_sub = pd.read_csv(INPUT / \"sample_submit.csv\", header=None)\n",
    "df_sample_sub.columns = [\"index\", \"genre\"]\n",
    "df_genre_labels = pd.read_csv(INPUT / \"genre_labels.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GroupFeatureExtractor:  # 参考: https://signate.jp/competitions/449/discussions/lgbm-baseline-lb06240\n",
    "    EX_TRANS_METHODS = [\"deviation\", \"zscore\"]\n",
    "    def __init__(self, group_key, group_values, agg_methods):\n",
    "        self.group_key = group_key\n",
    "        self.group_values = group_values\n",
    "        self.ex_trans_methods = [m for m in agg_methods if m in self.EX_TRANS_METHODS]\n",
    "        self.agg_methods = [m for m in agg_methods if m not in self.ex_trans_methods]\n",
    "        self.df_agg = None\n",
    "    def fit(self, df_train, y=None):\n",
    "        if not self.agg_methods:\n",
    "            return\n",
    "        dfs = []\n",
    "        for agg_method in self.agg_methods:\n",
    "            if callable(agg_method):\n",
    "                agg_method_name = agg_method.__name__\n",
    "            else:\n",
    "                agg_method_name = agg_method\n",
    "            df_agg = (df_train[[self.group_key] + self.group_values].groupby(self.group_key).agg(agg_method))\n",
    "            df_agg.columns = self._get_column_names(agg_method_name)\n",
    "            dfs.append(df_agg)\n",
    "        self.df_agg = pd.concat(dfs, axis=1).reset_index()\n",
    "    def transform(self, df_eval):\n",
    "        key = self.group_key\n",
    "        if self.agg_methods:\n",
    "            df_features = pd.merge(df_eval[[self.group_key]], self.df_agg, on=self.group_key, how=\"left\")\n",
    "        else:\n",
    "            df_features = df_eval[[self.group_key]].copy()\n",
    "        if self.ex_trans_methods:\n",
    "            if \"deviation\" in self.ex_trans_methods:\n",
    "                df_features[self._get_agg_column_names(\"deviation\")] = df_eval[self.group_values] - df_eval[[key]+self.group_values].groupby(key).transform(\"mean\")\n",
    "            if \"zscore\" in self.ex_trans_methods:\n",
    "                df_features[self._get_column_names(\"zscore\")] = (df_eval[self.group_values] - df_eval[[key]+self.group_values].groupby(key).transform(\"mean\")) \\\n",
    "                                                                / (df_eval[[key]+self.group_values].groupby(key).transform(\"std\") + 1e-8)\n",
    "        df_features.drop(self.group_key, axis=1, inplace=True)\n",
    "        return df_features\n",
    "    def _get_column_names(self, method):\n",
    "        return [f\"agg_{method}_{col}_grpby_{self.group_key}\" for col in self.group_values]\n",
    "    def fit_transform(self, df_train, y=None):\n",
    "        self.fit(df_train, y=y)\n",
    "        return self.transform(df_train) \n",
    "    \n",
    "    \n",
    "class KNNFeatureExtractor:\n",
    "    def __init__(self, n_neighbors=5):\n",
    "        self.knn = KNeighborsClassifier(n_neighbors + 1)\n",
    "    def fit(self, X, y):\n",
    "        self.knn.fit(X, y)\n",
    "        self.y = y if isinstance(y, np.ndarray) else np.array(y)\n",
    "        return self\n",
    "    def transform(self, X, is_train_data):\n",
    "        distances, indexes = self.knn.kneighbors(X)\n",
    "        distances = distances[:, 1:] if is_train_data else distances[:, :-1]\n",
    "        indexes = indexes[:, 1:] if is_train_data else indexes[:, :-1]\n",
    "        labels = self.y[indexes]\n",
    "        score_columns = [f\"knn_score_class{c:02d}\" for c in range(N_CLASSES)]\n",
    "        df_knn = pd.DataFrame(\n",
    "            [np.bincount(labels_, distances_, N_CLASSES) for labels_, distances_ in zip(labels, 1.0 / distances)],\n",
    "            columns=score_columns\n",
    "        )\n",
    "        # 最大スコア\n",
    "        df_knn['max_knn_scores'] = df_knn.max(axis=1)\n",
    "\n",
    "        # 最大スコアとの差。0は最大スコアを表す\n",
    "        for col in score_columns:\n",
    "            df_knn[f'sub_max_knn_scores_{col}'] = df_knn['max_knn_scores'] - df_knn[col]\n",
    "\n",
    "        # 最大スコアとの比。1は最大スコアを表す\n",
    "        for col in score_columns:\n",
    "            df_knn[f'div_max_knn_scores_{col}'] = df_knn[col] / df_knn['max_knn_scores']\n",
    "\n",
    "        # それぞれのスコア同士の差\n",
    "        for i, col1 in enumerate(score_columns):\n",
    "            for j, col2 in enumerate(score_columns[i+1:], i+1): # 全パターンを網羅できる\n",
    "                df_knn[f'sub_{col1}_{col2}'] = df_knn[col1] - df_knn[col2]\n",
    "\n",
    "        # knnスコアの合計\n",
    "        df_knn['sum_knn_scores'] = df_knn.sum(axis=1)\n",
    "\n",
    "        return df_knn\n",
    "# parameters - knn feature weights\n",
    "\n",
    "knn_features = [\n",
    "   'region_A', 'region_B', 'region_C', 'region_D', 'region_E', 'region_F',\n",
    "   'region_G', 'region_H', 'region_I', 'region_J', 'region_K', 'region_L',\n",
    "   'region_M', 'region_N', 'region_O', 'region_P', 'region_Q', 'region_R',\n",
    "   'region_S', 'region_T', 'region_unknown',\n",
    "   'standardscaled_popularity', 'standardscaled_duration_ms',\n",
    "   'standardscaled_acousticness', 'standardscaled_positiveness',\n",
    "   'standardscaled_danceability', 'standardscaled_loudness',\n",
    "   'standardscaled_energy', 'standardscaled_liveness',\n",
    "   'standardscaled_speechiness', 'standardscaled_instrumentalness',\n",
    "   'standardscaled_log_tempo', 'standardscaled_num_nans'\n",
    "]\n",
    "\n",
    "dict_feature_weights = {}\n",
    "\n",
    "for col in [\n",
    "    'region_A', 'region_B', 'region_C', 'region_D', 'region_E', 'region_F',\n",
    "    'region_G', 'region_H', 'region_I', 'region_J', 'region_K', 'region_L',\n",
    "    'region_M', 'region_N', 'region_O', 'region_P', 'region_Q', 'region_R',\n",
    "    'region_S', 'region_T', 'region_unknown'\n",
    "]:\n",
    "    dict_feature_weights[col] = 100.0\n",
    "\n",
    "for col in [\n",
    "    'standardscaled_duration_ms',\n",
    "    'standardscaled_acousticness', 'standardscaled_positiveness',\n",
    "    'standardscaled_danceability', 'standardscaled_loudness',\n",
    "    'standardscaled_energy', 'standardscaled_liveness',\n",
    "    'standardscaled_speechiness', 'standardscaled_instrumentalness'\n",
    "]:\n",
    "    dict_feature_weights[col] = 1.0\n",
    "\n",
    "dict_feature_weights[\"standardscaled_popularity\"] = 8.0\n",
    "dict_feature_weights[\"standardscaled_log_tempo\"] = 0.001\n",
    "dict_feature_weights[\"standardscaled_num_nans\"] = 100.0\n",
    "\n",
    "knn_feature_weights = np.array([dict_feature_weights[col] for col in knn_features])\n",
    "\n",
    "# parameters\n",
    "\n",
    "# def lgb_metric(preds, data):  \n",
    "#     pred_labels = preds.reshape(N_CLASSES, -1).argmax(axis=0)\n",
    "#     score = f1_score(data.get_label(), pred_labels, average=\"macro\")\n",
    "#     return \"macro_f1\", score, True\n",
    "\n",
    "learning_rate = 0.01\n",
    "lgb_params = {\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"num_class\": N_CLASSES,\n",
    "    #\"metric\": \"None\",\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"num_leaves\": 3,\n",
    "    \"min_data_in_leaf\": 40,\n",
    "    #\"colsample_bytree\": 1.0,\n",
    "    #\"feature_fraction\": 1.0,\n",
    "    #\"bagging_freq\": 0,\n",
    "    #\"bagging_fraction\": 1.0,\n",
    "    \"verbosity\": 0,\n",
    "    \"seed\": 42,\n",
    "    \"force_col_wise\":True\n",
    "}\n",
    "\n",
    "knn_n_neighbors = 6\n",
    "\n",
    "df_main = merge_train_test(df_train, df_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ede50b4f-a9e6-482a-b844-67cdfe5c10de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb68126-febd-4259-b8b1-17b72b6750d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ fold 0 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.665694\tvalid_1's multi_logloss: 0.695353\n",
      "[600]\ttraining's multi_logloss: 0.590728\tvalid_1's multi_logloss: 0.676796\n",
      "[900]\ttraining's multi_logloss: 0.542854\tvalid_1's multi_logloss: 0.676719\n",
      "Early stopping, best iteration is:\n",
      "[711]\ttraining's multi_logloss: 0.571438\tvalid_1's multi_logloss: 0.675461\n",
      "\n",
      "------------------------------ fold 1 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.664605\tvalid_1's multi_logloss: 0.646881\n",
      "[600]\ttraining's multi_logloss: 0.588284\tvalid_1's multi_logloss: 0.62156\n",
      "[900]\ttraining's multi_logloss: 0.537728\tvalid_1's multi_logloss: 0.619386\n",
      "[1200]\ttraining's multi_logloss: 0.498112\tvalid_1's multi_logloss: 0.620138\n",
      "Early stopping, best iteration is:\n",
      "[968]\ttraining's multi_logloss: 0.528\tvalid_1's multi_logloss: 0.618547\n",
      "\n",
      "------------------------------ fold 2 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.651067\tvalid_1's multi_logloss: 0.821327\n",
      "[600]\ttraining's multi_logloss: 0.575389\tvalid_1's multi_logloss: 0.808505\n",
      "[900]\ttraining's multi_logloss: 0.526331\tvalid_1's multi_logloss: 0.813431\n",
      "Early stopping, best iteration is:\n",
      "[638]\ttraining's multi_logloss: 0.568237\tvalid_1's multi_logloss: 0.808057\n",
      "\n",
      "------------------------------ fold 3 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.656075\tvalid_1's multi_logloss: 0.793407\n",
      "[600]\ttraining's multi_logloss: 0.576578\tvalid_1's multi_logloss: 0.793796\n",
      "Early stopping, best iteration is:\n",
      "[419]\ttraining's multi_logloss: 0.617783\tvalid_1's multi_logloss: 0.789894\n",
      "\n",
      "------------------------------ fold 4 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.658408\tvalid_1's multi_logloss: 0.783048\n",
      "[600]\ttraining's multi_logloss: 0.584655\tvalid_1's multi_logloss: 0.764774\n",
      "[900]\ttraining's multi_logloss: 0.536271\tvalid_1's multi_logloss: 0.760143\n",
      "[1200]\ttraining's multi_logloss: 0.497895\tvalid_1's multi_logloss: 0.761995\n",
      "Early stopping, best iteration is:\n",
      "[944]\ttraining's multi_logloss: 0.530147\tvalid_1's multi_logloss: 0.759822\n",
      "\n",
      "------------------------------ fold 5 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.664616\tvalid_1's multi_logloss: 0.75605\n",
      "[600]\ttraining's multi_logloss: 0.589566\tvalid_1's multi_logloss: 0.738289\n",
      "[900]\ttraining's multi_logloss: 0.541992\tvalid_1's multi_logloss: 0.736565\n",
      "Early stopping, best iteration is:\n",
      "[790]\ttraining's multi_logloss: 0.558023\tvalid_1's multi_logloss: 0.735685\n",
      "\n",
      "------------------------------ fold 6 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.66098\tvalid_1's multi_logloss: 0.74449\n",
      "[600]\ttraining's multi_logloss: 0.584753\tvalid_1's multi_logloss: 0.719007\n",
      "[900]\ttraining's multi_logloss: 0.536874\tvalid_1's multi_logloss: 0.7158\n",
      "[1200]\ttraining's multi_logloss: 0.497274\tvalid_1's multi_logloss: 0.714426\n",
      "[1500]\ttraining's multi_logloss: 0.464202\tvalid_1's multi_logloss: 0.716341\n",
      "Early stopping, best iteration is:\n",
      "[1267]\ttraining's multi_logloss: 0.489504\tvalid_1's multi_logloss: 0.713674\n",
      "\n",
      "------------------------------ fold 7 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.654402\tvalid_1's multi_logloss: 0.749377\n",
      "[600]\ttraining's multi_logloss: 0.576059\tvalid_1's multi_logloss: 0.732406\n",
      "[900]\ttraining's multi_logloss: 0.52623\tvalid_1's multi_logloss: 0.728374\n",
      "[1200]\ttraining's multi_logloss: 0.486912\tvalid_1's multi_logloss: 0.726845\n",
      "Early stopping, best iteration is:\n",
      "[1080]\ttraining's multi_logloss: 0.501887\tvalid_1's multi_logloss: 0.726413\n",
      "\n",
      "------------------------------ fold 8 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.664787\tvalid_1's multi_logloss: 0.699309\n",
      "[600]\ttraining's multi_logloss: 0.589501\tvalid_1's multi_logloss: 0.690215\n",
      "Early stopping, best iteration is:\n",
      "[510]\ttraining's multi_logloss: 0.60757\tvalid_1's multi_logloss: 0.688757\n",
      "\n",
      "------------------------------ fold 9 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.660071\tvalid_1's multi_logloss: 0.757835\n",
      "[600]\ttraining's multi_logloss: 0.585045\tvalid_1's multi_logloss: 0.737641\n",
      "[900]\ttraining's multi_logloss: 0.535642\tvalid_1's multi_logloss: 0.733006\n",
      "Early stopping, best iteration is:\n",
      "[840]\ttraining's multi_logloss: 0.544582\tvalid_1's multi_logloss: 0.73152\n",
      "\n",
      "------------------------------ fold 10 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.670643\tvalid_1's multi_logloss: 0.682232\n",
      "[600]\ttraining's multi_logloss: 0.593853\tvalid_1's multi_logloss: 0.654242\n",
      "[900]\ttraining's multi_logloss: 0.544896\tvalid_1's multi_logloss: 0.646393\n",
      "[1200]\ttraining's multi_logloss: 0.506305\tvalid_1's multi_logloss: 0.642717\n",
      "[1500]\ttraining's multi_logloss: 0.47332\tvalid_1's multi_logloss: 0.643249\n",
      "Early stopping, best iteration is:\n",
      "[1315]\ttraining's multi_logloss: 0.493154\tvalid_1's multi_logloss: 0.641642\n",
      "\n",
      "------------------------------ fold 11 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.651247\tvalid_1's multi_logloss: 0.779002\n",
      "[600]\ttraining's multi_logloss: 0.574521\tvalid_1's multi_logloss: 0.763616\n",
      "[900]\ttraining's multi_logloss: 0.524599\tvalid_1's multi_logloss: 0.765682\n",
      "Early stopping, best iteration is:\n",
      "[642]\ttraining's multi_logloss: 0.566582\tvalid_1's multi_logloss: 0.762819\n",
      "\n",
      "------------------------------ fold 12 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.656631\tvalid_1's multi_logloss: 0.727327\n",
      "[600]\ttraining's multi_logloss: 0.582672\tvalid_1's multi_logloss: 0.703406\n",
      "[900]\ttraining's multi_logloss: 0.533704\tvalid_1's multi_logloss: 0.697785\n",
      "[1200]\ttraining's multi_logloss: 0.493552\tvalid_1's multi_logloss: 0.693444\n",
      "[1500]\ttraining's multi_logloss: 0.459428\tvalid_1's multi_logloss: 0.692698\n",
      "[1800]\ttraining's multi_logloss: 0.428907\tvalid_1's multi_logloss: 0.693495\n",
      "Early stopping, best iteration is:\n",
      "[1537]\ttraining's multi_logloss: 0.455517\tvalid_1's multi_logloss: 0.692477\n",
      "\n",
      "------------------------------ fold 13 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.660783\tvalid_1's multi_logloss: 0.785841\n",
      "[600]\ttraining's multi_logloss: 0.58422\tvalid_1's multi_logloss: 0.759897\n",
      "[900]\ttraining's multi_logloss: 0.534432\tvalid_1's multi_logloss: 0.756157\n",
      "[1200]\ttraining's multi_logloss: 0.495576\tvalid_1's multi_logloss: 0.756807\n",
      "Early stopping, best iteration is:\n",
      "[1092]\ttraining's multi_logloss: 0.508653\tvalid_1's multi_logloss: 0.755239\n",
      "\n",
      "------------------------------ fold 14 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.656316\tvalid_1's multi_logloss: 0.82472\n",
      "[600]\ttraining's multi_logloss: 0.581159\tvalid_1's multi_logloss: 0.801156\n",
      "[900]\ttraining's multi_logloss: 0.531711\tvalid_1's multi_logloss: 0.79347\n",
      "[1200]\ttraining's multi_logloss: 0.492669\tvalid_1's multi_logloss: 0.793743\n",
      "Early stopping, best iteration is:\n",
      "[938]\ttraining's multi_logloss: 0.526332\tvalid_1's multi_logloss: 0.79288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [07:59<39:57, 479.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV score (not reliable!)\n",
      "  f1:  0.68150\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.74      0.78        42\n",
      "           1       0.54      0.40      0.46       207\n",
      "           2       0.74      0.60      0.66       193\n",
      "           3       0.86      0.83      0.84       464\n",
      "           4       0.67      0.58      0.62        45\n",
      "           5       0.65      0.53      0.59       131\n",
      "           6       0.58      0.36      0.44        50\n",
      "           7       0.63      0.62      0.63       334\n",
      "           8       0.72      0.79      0.76      1326\n",
      "           9       0.91      0.88      0.90        85\n",
      "          10       0.82      0.84      0.83      1656\n",
      "\n",
      "    accuracy                           0.76      4533\n",
      "   macro avg       0.72      0.65      0.68      4533\n",
      "weighted avg       0.76      0.76      0.76      4533\n",
      "\n",
      "184 rows were filled. (confidence>0.95)\n",
      "filled test labels: [  1   0   2  24   0   3   0   0  15  13 126]\n",
      "\n",
      "\n",
      "------------------------------ fold 0 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.634723\tvalid_1's multi_logloss: 0.716375\n",
      "[600]\ttraining's multi_logloss: 0.559711\tvalid_1's multi_logloss: 0.699869\n",
      "[900]\ttraining's multi_logloss: 0.511387\tvalid_1's multi_logloss: 0.699905\n",
      "Early stopping, best iteration is:\n",
      "[786]\ttraining's multi_logloss: 0.528049\tvalid_1's multi_logloss: 0.698543\n",
      "\n",
      "------------------------------ fold 1 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.640467\tvalid_1's multi_logloss: 0.643417\n",
      "[600]\ttraining's multi_logloss: 0.565486\tvalid_1's multi_logloss: 0.621117\n",
      "[900]\ttraining's multi_logloss: 0.517208\tvalid_1's multi_logloss: 0.618417\n",
      "[1200]\ttraining's multi_logloss: 0.478687\tvalid_1's multi_logloss: 0.619185\n",
      "Early stopping, best iteration is:\n",
      "[1019]\ttraining's multi_logloss: 0.500888\tvalid_1's multi_logloss: 0.617834\n",
      "\n",
      "------------------------------ fold 2 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.626281\tvalid_1's multi_logloss: 0.742297\n",
      "[600]\ttraining's multi_logloss: 0.553295\tvalid_1's multi_logloss: 0.714685\n",
      "[900]\ttraining's multi_logloss: 0.505645\tvalid_1's multi_logloss: 0.714706\n",
      "Early stopping, best iteration is:\n",
      "[670]\ttraining's multi_logloss: 0.540669\tvalid_1's multi_logloss: 0.713635\n",
      "\n",
      "------------------------------ fold 3 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.63882\tvalid_1's multi_logloss: 0.636602\n",
      "[600]\ttraining's multi_logloss: 0.565854\tvalid_1's multi_logloss: 0.62629\n",
      "Early stopping, best iteration is:\n",
      "[547]\ttraining's multi_logloss: 0.575904\tvalid_1's multi_logloss: 0.625491\n",
      "\n",
      "------------------------------ fold 4 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.639946\tvalid_1's multi_logloss: 0.733254\n",
      "[600]\ttraining's multi_logloss: 0.567002\tvalid_1's multi_logloss: 0.705657\n",
      "[900]\ttraining's multi_logloss: 0.518635\tvalid_1's multi_logloss: 0.699366\n",
      "[1200]\ttraining's multi_logloss: 0.480257\tvalid_1's multi_logloss: 0.696129\n",
      "[1500]\ttraining's multi_logloss: 0.447668\tvalid_1's multi_logloss: 0.697349\n",
      "Early stopping, best iteration is:\n",
      "[1315]\ttraining's multi_logloss: 0.467419\tvalid_1's multi_logloss: 0.695538\n",
      "\n",
      "------------------------------ fold 5 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.641839\tvalid_1's multi_logloss: 0.667606\n",
      "[600]\ttraining's multi_logloss: 0.567821\tvalid_1's multi_logloss: 0.646554\n",
      "[900]\ttraining's multi_logloss: 0.519481\tvalid_1's multi_logloss: 0.646922\n",
      "Early stopping, best iteration is:\n",
      "[790]\ttraining's multi_logloss: 0.535704\tvalid_1's multi_logloss: 0.645387\n",
      "\n",
      "------------------------------ fold 6 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.638331\tvalid_1's multi_logloss: 0.730229\n",
      "[600]\ttraining's multi_logloss: 0.566709\tvalid_1's multi_logloss: 0.701269\n",
      "[900]\ttraining's multi_logloss: 0.52126\tvalid_1's multi_logloss: 0.697723\n",
      "[1200]\ttraining's multi_logloss: 0.483219\tvalid_1's multi_logloss: 0.697959\n",
      "Early stopping, best iteration is:\n",
      "[1015]\ttraining's multi_logloss: 0.506044\tvalid_1's multi_logloss: 0.697451\n",
      "\n",
      "------------------------------ fold 7 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.625778\tvalid_1's multi_logloss: 0.749985\n",
      "[600]\ttraining's multi_logloss: 0.552555\tvalid_1's multi_logloss: 0.731021\n",
      "[900]\ttraining's multi_logloss: 0.503955\tvalid_1's multi_logloss: 0.732889\n",
      "Early stopping, best iteration is:\n",
      "[610]\ttraining's multi_logloss: 0.55074\tvalid_1's multi_logloss: 0.730623\n",
      "\n",
      "------------------------------ fold 8 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.635117\tvalid_1's multi_logloss: 0.757653\n",
      "[600]\ttraining's multi_logloss: 0.561524\tvalid_1's multi_logloss: 0.735926\n",
      "[900]\ttraining's multi_logloss: 0.513685\tvalid_1's multi_logloss: 0.729953\n",
      "[1200]\ttraining's multi_logloss: 0.475935\tvalid_1's multi_logloss: 0.727861\n",
      "Early stopping, best iteration is:\n",
      "[1185]\ttraining's multi_logloss: 0.477626\tvalid_1's multi_logloss: 0.727706\n",
      "\n",
      "------------------------------ fold 9 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.631102\tvalid_1's multi_logloss: 0.668999\n",
      "[600]\ttraining's multi_logloss: 0.557898\tvalid_1's multi_logloss: 0.648188\n",
      "[900]\ttraining's multi_logloss: 0.510563\tvalid_1's multi_logloss: 0.645356\n",
      "[1200]\ttraining's multi_logloss: 0.472657\tvalid_1's multi_logloss: 0.646969\n",
      "Early stopping, best iteration is:\n",
      "[920]\ttraining's multi_logloss: 0.507801\tvalid_1's multi_logloss: 0.645009\n",
      "\n",
      "------------------------------ fold 10 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.64031\tvalid_1's multi_logloss: 0.779151\n",
      "[600]\ttraining's multi_logloss: 0.565069\tvalid_1's multi_logloss: 0.762446\n",
      "Early stopping, best iteration is:\n",
      "[556]\ttraining's multi_logloss: 0.573797\tvalid_1's multi_logloss: 0.761301\n",
      "\n",
      "------------------------------ fold 11 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.625485\tvalid_1's multi_logloss: 0.737463\n",
      "[600]\ttraining's multi_logloss: 0.552489\tvalid_1's multi_logloss: 0.726162\n",
      "[900]\ttraining's multi_logloss: 0.50205\tvalid_1's multi_logloss: 0.722701\n",
      "[1200]\ttraining's multi_logloss: 0.462404\tvalid_1's multi_logloss: 0.726398\n",
      "Early stopping, best iteration is:\n",
      "[905]\ttraining's multi_logloss: 0.501357\tvalid_1's multi_logloss: 0.722607\n",
      "\n",
      "------------------------------ fold 12 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.625522\tvalid_1's multi_logloss: 0.787965\n",
      "[600]\ttraining's multi_logloss: 0.550917\tvalid_1's multi_logloss: 0.777514\n",
      "Early stopping, best iteration is:\n",
      "[583]\ttraining's multi_logloss: 0.554051\tvalid_1's multi_logloss: 0.777073\n",
      "\n",
      "------------------------------ fold 13 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.638717\tvalid_1's multi_logloss: 0.741958\n",
      "[600]\ttraining's multi_logloss: 0.565017\tvalid_1's multi_logloss: 0.713831\n",
      "[900]\ttraining's multi_logloss: 0.516419\tvalid_1's multi_logloss: 0.709251\n",
      "[1200]\ttraining's multi_logloss: 0.477839\tvalid_1's multi_logloss: 0.707575\n",
      "[1500]\ttraining's multi_logloss: 0.444246\tvalid_1's multi_logloss: 0.707516\n",
      "Early stopping, best iteration is:\n",
      "[1344]\ttraining's multi_logloss: 0.461293\tvalid_1's multi_logloss: 0.706977\n",
      "\n",
      "------------------------------ fold 14 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.640713\tvalid_1's multi_logloss: 0.701328\n",
      "[600]\ttraining's multi_logloss: 0.567839\tvalid_1's multi_logloss: 0.690309\n",
      "[900]\ttraining's multi_logloss: 0.521109\tvalid_1's multi_logloss: 0.691451\n",
      "Early stopping, best iteration is:\n",
      "[677]\ttraining's multi_logloss: 0.554651\tvalid_1's multi_logloss: 0.689473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [15:25<30:39, 459.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV score (not reliable!)\n",
      "  f1:  0.68426\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.72      0.76        43\n",
      "           1       0.57      0.46      0.51       207\n",
      "           2       0.73      0.59      0.65       195\n",
      "           3       0.87      0.83      0.85       488\n",
      "           4       0.68      0.51      0.58        45\n",
      "           5       0.66      0.53      0.59       134\n",
      "           6       0.53      0.40      0.45        50\n",
      "           7       0.64      0.60      0.62       334\n",
      "           8       0.72      0.80      0.76      1341\n",
      "           9       0.91      0.91      0.91        98\n",
      "          10       0.84      0.85      0.85      1782\n",
      "\n",
      "    accuracy                           0.77      4717\n",
      "   macro avg       0.72      0.66      0.68      4717\n",
      "weighted avg       0.77      0.77      0.77      4717\n",
      "\n",
      "341 rows were filled. (confidence>0.925)\n",
      "filled test labels: [  4   3  12  46   1  11   0   0 138   0 126]\n",
      "\n",
      "\n",
      "------------------------------ fold 0 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.600469\tvalid_1's multi_logloss: 0.636846\n",
      "[600]\ttraining's multi_logloss: 0.531578\tvalid_1's multi_logloss: 0.615537\n",
      "[900]\ttraining's multi_logloss: 0.485831\tvalid_1's multi_logloss: 0.612869\n",
      "[1200]\ttraining's multi_logloss: 0.449901\tvalid_1's multi_logloss: 0.613369\n",
      "Early stopping, best iteration is:\n",
      "[1047]\ttraining's multi_logloss: 0.467413\tvalid_1's multi_logloss: 0.6119\n",
      "\n",
      "------------------------------ fold 1 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.590948\tvalid_1's multi_logloss: 0.638495\n",
      "[600]\ttraining's multi_logloss: 0.521101\tvalid_1's multi_logloss: 0.614897\n",
      "[900]\ttraining's multi_logloss: 0.47566\tvalid_1's multi_logloss: 0.609489\n",
      "[1200]\ttraining's multi_logloss: 0.43926\tvalid_1's multi_logloss: 0.606426\n",
      "[1500]\ttraining's multi_logloss: 0.408384\tvalid_1's multi_logloss: 0.604138\n",
      "[1800]\ttraining's multi_logloss: 0.381002\tvalid_1's multi_logloss: 0.604413\n",
      "Early stopping, best iteration is:\n",
      "[1644]\ttraining's multi_logloss: 0.395185\tvalid_1's multi_logloss: 0.603801\n",
      "\n",
      "------------------------------ fold 2 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.593197\tvalid_1's multi_logloss: 0.710767\n",
      "[600]\ttraining's multi_logloss: 0.522104\tvalid_1's multi_logloss: 0.698335\n",
      "Early stopping, best iteration is:\n",
      "[538]\ttraining's multi_logloss: 0.533457\tvalid_1's multi_logloss: 0.697599\n",
      "\n",
      "------------------------------ fold 3 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.598821\tvalid_1's multi_logloss: 0.678138\n",
      "[600]\ttraining's multi_logloss: 0.529146\tvalid_1's multi_logloss: 0.670631\n",
      "Early stopping, best iteration is:\n",
      "[482]\ttraining's multi_logloss: 0.551718\tvalid_1's multi_logloss: 0.669331\n",
      "\n",
      "------------------------------ fold 4 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.590898\tvalid_1's multi_logloss: 0.725399\n",
      "[600]\ttraining's multi_logloss: 0.523207\tvalid_1's multi_logloss: 0.709543\n",
      "Early stopping, best iteration is:\n",
      "[576]\ttraining's multi_logloss: 0.527236\tvalid_1's multi_logloss: 0.709346\n",
      "\n",
      "------------------------------ fold 5 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.598929\tvalid_1's multi_logloss: 0.692459\n",
      "[600]\ttraining's multi_logloss: 0.528213\tvalid_1's multi_logloss: 0.681755\n",
      "Early stopping, best iteration is:\n",
      "[581]\ttraining's multi_logloss: 0.531626\tvalid_1's multi_logloss: 0.681393\n",
      "\n",
      "------------------------------ fold 6 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.603492\tvalid_1's multi_logloss: 0.656077\n",
      "[600]\ttraining's multi_logloss: 0.533223\tvalid_1's multi_logloss: 0.637968\n",
      "[900]\ttraining's multi_logloss: 0.487745\tvalid_1's multi_logloss: 0.63925\n",
      "Early stopping, best iteration is:\n",
      "[729]\ttraining's multi_logloss: 0.512258\tvalid_1's multi_logloss: 0.637596\n",
      "\n",
      "------------------------------ fold 7 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.595533\tvalid_1's multi_logloss: 0.65263\n",
      "[600]\ttraining's multi_logloss: 0.524016\tvalid_1's multi_logloss: 0.627559\n",
      "[900]\ttraining's multi_logloss: 0.47732\tvalid_1's multi_logloss: 0.622735\n",
      "[1200]\ttraining's multi_logloss: 0.440979\tvalid_1's multi_logloss: 0.622836\n",
      "Early stopping, best iteration is:\n",
      "[1005]\ttraining's multi_logloss: 0.46391\tvalid_1's multi_logloss: 0.621876\n",
      "\n",
      "------------------------------ fold 8 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.5932\tvalid_1's multi_logloss: 0.70071\n",
      "[600]\ttraining's multi_logloss: 0.522133\tvalid_1's multi_logloss: 0.67818\n",
      "[900]\ttraining's multi_logloss: 0.477227\tvalid_1's multi_logloss: 0.67631\n",
      "Early stopping, best iteration is:\n",
      "[855]\ttraining's multi_logloss: 0.483281\tvalid_1's multi_logloss: 0.676028\n",
      "\n",
      "------------------------------ fold 9 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.596669\tvalid_1's multi_logloss: 0.653929\n",
      "[600]\ttraining's multi_logloss: 0.527754\tvalid_1's multi_logloss: 0.630802\n",
      "[900]\ttraining's multi_logloss: 0.481698\tvalid_1's multi_logloss: 0.62908\n",
      "Early stopping, best iteration is:\n",
      "[781]\ttraining's multi_logloss: 0.498674\tvalid_1's multi_logloss: 0.628521\n",
      "\n",
      "------------------------------ fold 10 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.5939\tvalid_1's multi_logloss: 0.775256\n",
      "[600]\ttraining's multi_logloss: 0.523994\tvalid_1's multi_logloss: 0.75937\n",
      "[900]\ttraining's multi_logloss: 0.480641\tvalid_1's multi_logloss: 0.758282\n",
      "Early stopping, best iteration is:\n",
      "[700]\ttraining's multi_logloss: 0.507797\tvalid_1's multi_logloss: 0.756091\n",
      "\n",
      "------------------------------ fold 11 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.594263\tvalid_1's multi_logloss: 0.659379\n",
      "[600]\ttraining's multi_logloss: 0.52238\tvalid_1's multi_logloss: 0.645688\n",
      "[900]\ttraining's multi_logloss: 0.477554\tvalid_1's multi_logloss: 0.643386\n",
      "[1200]\ttraining's multi_logloss: 0.441948\tvalid_1's multi_logloss: 0.643672\n",
      "Early stopping, best iteration is:\n",
      "[1015]\ttraining's multi_logloss: 0.463026\tvalid_1's multi_logloss: 0.642543\n",
      "\n",
      "------------------------------ fold 12 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.592333\tvalid_1's multi_logloss: 0.645499\n",
      "[600]\ttraining's multi_logloss: 0.523125\tvalid_1's multi_logloss: 0.623811\n",
      "[900]\ttraining's multi_logloss: 0.477906\tvalid_1's multi_logloss: 0.620082\n",
      "Early stopping, best iteration is:\n",
      "[826]\ttraining's multi_logloss: 0.48808\tvalid_1's multi_logloss: 0.619838\n",
      "\n",
      "------------------------------ fold 13 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.594902\tvalid_1's multi_logloss: 0.675013\n",
      "[600]\ttraining's multi_logloss: 0.523958\tvalid_1's multi_logloss: 0.658101\n",
      "[900]\ttraining's multi_logloss: 0.478646\tvalid_1's multi_logloss: 0.651435\n",
      "[1200]\ttraining's multi_logloss: 0.443699\tvalid_1's multi_logloss: 0.651906\n",
      "Early stopping, best iteration is:\n",
      "[993]\ttraining's multi_logloss: 0.466931\tvalid_1's multi_logloss: 0.650775\n",
      "\n",
      "------------------------------ fold 14 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.598378\tvalid_1's multi_logloss: 0.626248\n",
      "[600]\ttraining's multi_logloss: 0.52952\tvalid_1's multi_logloss: 0.607757\n",
      "[900]\ttraining's multi_logloss: 0.485052\tvalid_1's multi_logloss: 0.602047\n",
      "[1200]\ttraining's multi_logloss: 0.449183\tvalid_1's multi_logloss: 0.60298\n",
      "Early stopping, best iteration is:\n",
      "[1032]\ttraining's multi_logloss: 0.468497\tvalid_1's multi_logloss: 0.601122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [22:56<22:46, 455.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV score (not reliable!)\n",
      "  f1:  0.70433\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.74      0.79        47\n",
      "           1       0.57      0.46      0.51       210\n",
      "           2       0.73      0.59      0.65       207\n",
      "           3       0.88      0.85      0.86       534\n",
      "           4       0.76      0.63      0.69        46\n",
      "           5       0.69      0.53      0.60       145\n",
      "           6       0.71      0.34      0.46        50\n",
      "           7       0.64      0.62      0.63       334\n",
      "           8       0.75      0.82      0.78      1479\n",
      "           9       0.93      0.91      0.92        98\n",
      "          10       0.85      0.86      0.86      1908\n",
      "\n",
      "    accuracy                           0.79      5058\n",
      "   macro avg       0.76      0.67      0.70      5058\n",
      "weighted avg       0.79      0.79      0.79      5058\n",
      "\n",
      "345 rows were filled. (confidence>0.9)\n",
      "filled test labels: [  4   2   8  47   2   3   1   3 180   1  94]\n",
      "\n",
      "\n",
      "------------------------------ fold 0 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.56641\tvalid_1's multi_logloss: 0.579289\n",
      "[600]\ttraining's multi_logloss: 0.50074\tvalid_1's multi_logloss: 0.549118\n",
      "[900]\ttraining's multi_logloss: 0.457989\tvalid_1's multi_logloss: 0.54558\n",
      "Early stopping, best iteration is:\n",
      "[848]\ttraining's multi_logloss: 0.464625\tvalid_1's multi_logloss: 0.545399\n",
      "\n",
      "------------------------------ fold 1 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.557939\tvalid_1's multi_logloss: 0.586585\n",
      "[600]\ttraining's multi_logloss: 0.491256\tvalid_1's multi_logloss: 0.559921\n",
      "[900]\ttraining's multi_logloss: 0.447665\tvalid_1's multi_logloss: 0.55642\n",
      "[1200]\ttraining's multi_logloss: 0.413985\tvalid_1's multi_logloss: 0.557886\n",
      "Early stopping, best iteration is:\n",
      "[920]\ttraining's multi_logloss: 0.445149\tvalid_1's multi_logloss: 0.55605\n",
      "\n",
      "------------------------------ fold 2 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.549231\tvalid_1's multi_logloss: 0.751703\n",
      "[600]\ttraining's multi_logloss: 0.484592\tvalid_1's multi_logloss: 0.728477\n",
      "[900]\ttraining's multi_logloss: 0.441529\tvalid_1's multi_logloss: 0.725924\n",
      "[1200]\ttraining's multi_logloss: 0.407086\tvalid_1's multi_logloss: 0.726683\n",
      "Early stopping, best iteration is:\n",
      "[1024]\ttraining's multi_logloss: 0.426757\tvalid_1's multi_logloss: 0.725649\n",
      "\n",
      "------------------------------ fold 3 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.561015\tvalid_1's multi_logloss: 0.603363\n",
      "[600]\ttraining's multi_logloss: 0.495089\tvalid_1's multi_logloss: 0.58119\n",
      "[900]\ttraining's multi_logloss: 0.451984\tvalid_1's multi_logloss: 0.575546\n",
      "[1200]\ttraining's multi_logloss: 0.416843\tvalid_1's multi_logloss: 0.575564\n",
      "[1500]\ttraining's multi_logloss: 0.387391\tvalid_1's multi_logloss: 0.575631\n",
      "Early stopping, best iteration is:\n",
      "[1402]\ttraining's multi_logloss: 0.396436\tvalid_1's multi_logloss: 0.574824\n",
      "\n",
      "------------------------------ fold 4 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.558987\tvalid_1's multi_logloss: 0.662798\n",
      "[600]\ttraining's multi_logloss: 0.492709\tvalid_1's multi_logloss: 0.649566\n",
      "Early stopping, best iteration is:\n",
      "[507]\ttraining's multi_logloss: 0.508928\tvalid_1's multi_logloss: 0.648648\n",
      "\n",
      "------------------------------ fold 5 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.560051\tvalid_1's multi_logloss: 0.675643\n",
      "[600]\ttraining's multi_logloss: 0.492875\tvalid_1's multi_logloss: 0.650732\n",
      "[900]\ttraining's multi_logloss: 0.4514\tvalid_1's multi_logloss: 0.646233\n",
      "[1200]\ttraining's multi_logloss: 0.418835\tvalid_1's multi_logloss: 0.647129\n",
      "Early stopping, best iteration is:\n",
      "[1034]\ttraining's multi_logloss: 0.43611\tvalid_1's multi_logloss: 0.645871\n",
      "\n",
      "------------------------------ fold 6 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.561072\tvalid_1's multi_logloss: 0.700989\n",
      "[600]\ttraining's multi_logloss: 0.496257\tvalid_1's multi_logloss: 0.677004\n",
      "[900]\ttraining's multi_logloss: 0.454455\tvalid_1's multi_logloss: 0.674088\n",
      "[1200]\ttraining's multi_logloss: 0.421309\tvalid_1's multi_logloss: 0.672979\n",
      "Early stopping, best iteration is:\n",
      "[1110]\ttraining's multi_logloss: 0.430551\tvalid_1's multi_logloss: 0.672241\n",
      "\n",
      "------------------------------ fold 7 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.555426\tvalid_1's multi_logloss: 0.64436\n",
      "[600]\ttraining's multi_logloss: 0.488079\tvalid_1's multi_logloss: 0.628213\n",
      "Early stopping, best iteration is:\n",
      "[571]\ttraining's multi_logloss: 0.493076\tvalid_1's multi_logloss: 0.627769\n",
      "\n",
      "------------------------------ fold 8 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.556953\tvalid_1's multi_logloss: 0.644696\n",
      "[600]\ttraining's multi_logloss: 0.491006\tvalid_1's multi_logloss: 0.633957\n",
      "Early stopping, best iteration is:\n",
      "[532]\ttraining's multi_logloss: 0.502571\tvalid_1's multi_logloss: 0.632943\n",
      "\n",
      "------------------------------ fold 9 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.55583\tvalid_1's multi_logloss: 0.653099\n",
      "[600]\ttraining's multi_logloss: 0.488593\tvalid_1's multi_logloss: 0.64021\n",
      "[900]\ttraining's multi_logloss: 0.445798\tvalid_1's multi_logloss: 0.641463\n",
      "Early stopping, best iteration is:\n",
      "[713]\ttraining's multi_logloss: 0.471017\tvalid_1's multi_logloss: 0.638177\n",
      "\n",
      "------------------------------ fold 10 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.565119\tvalid_1's multi_logloss: 0.622958\n",
      "[600]\ttraining's multi_logloss: 0.499771\tvalid_1's multi_logloss: 0.607962\n",
      "[900]\ttraining's multi_logloss: 0.457677\tvalid_1's multi_logloss: 0.605047\n",
      "Early stopping, best iteration is:\n",
      "[882]\ttraining's multi_logloss: 0.459917\tvalid_1's multi_logloss: 0.604864\n",
      "\n",
      "------------------------------ fold 11 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.556087\tvalid_1's multi_logloss: 0.62342\n",
      "[600]\ttraining's multi_logloss: 0.489798\tvalid_1's multi_logloss: 0.602327\n",
      "[900]\ttraining's multi_logloss: 0.445584\tvalid_1's multi_logloss: 0.602794\n",
      "Early stopping, best iteration is:\n",
      "[725]\ttraining's multi_logloss: 0.469992\tvalid_1's multi_logloss: 0.600108\n",
      "\n",
      "------------------------------ fold 12 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.559104\tvalid_1's multi_logloss: 0.565826\n",
      "[600]\ttraining's multi_logloss: 0.490564\tvalid_1's multi_logloss: 0.556562\n",
      "[900]\ttraining's multi_logloss: 0.446756\tvalid_1's multi_logloss: 0.554341\n",
      "[1200]\ttraining's multi_logloss: 0.412635\tvalid_1's multi_logloss: 0.553706\n",
      "Early stopping, best iteration is:\n",
      "[1080]\ttraining's multi_logloss: 0.425448\tvalid_1's multi_logloss: 0.553075\n",
      "\n",
      "------------------------------ fold 13 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.56493\tvalid_1's multi_logloss: 0.603653\n",
      "[600]\ttraining's multi_logloss: 0.499606\tvalid_1's multi_logloss: 0.585035\n",
      "[900]\ttraining's multi_logloss: 0.457328\tvalid_1's multi_logloss: 0.583329\n",
      "[1200]\ttraining's multi_logloss: 0.423716\tvalid_1's multi_logloss: 0.58155\n",
      "Early stopping, best iteration is:\n",
      "[1181]\ttraining's multi_logloss: 0.425717\tvalid_1's multi_logloss: 0.581062\n",
      "\n",
      "------------------------------ fold 14 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.56433\tvalid_1's multi_logloss: 0.614515\n",
      "[600]\ttraining's multi_logloss: 0.498051\tvalid_1's multi_logloss: 0.59954\n",
      "[900]\ttraining's multi_logloss: 0.455218\tvalid_1's multi_logloss: 0.602472\n",
      "Early stopping, best iteration is:\n",
      "[683]\ttraining's multi_logloss: 0.485295\tvalid_1's multi_logloss: 0.599025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [30:11<14:55, 447.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV score (not reliable!)\n",
      "  f1:  0.70920\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.80      0.85        51\n",
      "           1       0.59      0.44      0.51       212\n",
      "           2       0.75      0.61      0.67       215\n",
      "           3       0.88      0.86      0.87       581\n",
      "           4       0.72      0.60      0.66        48\n",
      "           5       0.68      0.55      0.61       148\n",
      "           6       0.59      0.33      0.42        51\n",
      "           7       0.64      0.63      0.63       337\n",
      "           8       0.78      0.83      0.81      1659\n",
      "           9       0.92      0.90      0.91        99\n",
      "          10       0.85      0.87      0.86      2002\n",
      "\n",
      "    accuracy                           0.80      5403\n",
      "   macro avg       0.75      0.68      0.71      5403\n",
      "weighted avg       0.80      0.80      0.80      5403\n",
      "\n",
      "315 rows were filled. (confidence>0.875)\n",
      "filled test labels: [  0   5   6  30   5   6   0  16 167   3  77]\n",
      "\n",
      "\n",
      "------------------------------ fold 0 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.532466\tvalid_1's multi_logloss: 0.587441\n",
      "[600]\ttraining's multi_logloss: 0.469056\tvalid_1's multi_logloss: 0.579636\n",
      "Early stopping, best iteration is:\n",
      "[491]\ttraining's multi_logloss: 0.48773\tvalid_1's multi_logloss: 0.578428\n",
      "\n",
      "------------------------------ fold 1 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.524203\tvalid_1's multi_logloss: 0.665007\n",
      "[600]\ttraining's multi_logloss: 0.461133\tvalid_1's multi_logloss: 0.640564\n",
      "[900]\ttraining's multi_logloss: 0.420084\tvalid_1's multi_logloss: 0.635046\n",
      "[1200]\ttraining's multi_logloss: 0.387744\tvalid_1's multi_logloss: 0.631251\n",
      "[1500]\ttraining's multi_logloss: 0.360499\tvalid_1's multi_logloss: 0.630617\n",
      "[1800]\ttraining's multi_logloss: 0.33667\tvalid_1's multi_logloss: 0.631748\n",
      "Early stopping, best iteration is:\n",
      "[1521]\ttraining's multi_logloss: 0.358712\tvalid_1's multi_logloss: 0.630458\n",
      "\n",
      "------------------------------ fold 2 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.533036\tvalid_1's multi_logloss: 0.535694\n",
      "[600]\ttraining's multi_logloss: 0.469586\tvalid_1's multi_logloss: 0.515657\n",
      "[900]\ttraining's multi_logloss: 0.428493\tvalid_1's multi_logloss: 0.51708\n",
      "Early stopping, best iteration is:\n",
      "[643]\ttraining's multi_logloss: 0.46316\tvalid_1's multi_logloss: 0.515541\n",
      "\n",
      "------------------------------ fold 3 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.532486\tvalid_1's multi_logloss: 0.571241\n",
      "[600]\ttraining's multi_logloss: 0.46839\tvalid_1's multi_logloss: 0.559119\n",
      "[900]\ttraining's multi_logloss: 0.427206\tvalid_1's multi_logloss: 0.560079\n",
      "Early stopping, best iteration is:\n",
      "[630]\ttraining's multi_logloss: 0.463695\tvalid_1's multi_logloss: 0.558813\n",
      "\n",
      "------------------------------ fold 4 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.531581\tvalid_1's multi_logloss: 0.649571\n",
      "[600]\ttraining's multi_logloss: 0.46832\tvalid_1's multi_logloss: 0.636797\n",
      "[900]\ttraining's multi_logloss: 0.429376\tvalid_1's multi_logloss: 0.639878\n",
      "Early stopping, best iteration is:\n",
      "[619]\ttraining's multi_logloss: 0.465453\tvalid_1's multi_logloss: 0.636608\n",
      "\n",
      "------------------------------ fold 5 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.53717\tvalid_1's multi_logloss: 0.621729\n",
      "[600]\ttraining's multi_logloss: 0.474029\tvalid_1's multi_logloss: 0.602362\n",
      "[900]\ttraining's multi_logloss: 0.433592\tvalid_1's multi_logloss: 0.599843\n",
      "[1200]\ttraining's multi_logloss: 0.401159\tvalid_1's multi_logloss: 0.598735\n",
      "[1500]\ttraining's multi_logloss: 0.374418\tvalid_1's multi_logloss: 0.5992\n",
      "Early stopping, best iteration is:\n",
      "[1320]\ttraining's multi_logloss: 0.389969\tvalid_1's multi_logloss: 0.598404\n",
      "\n",
      "------------------------------ fold 6 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.527868\tvalid_1's multi_logloss: 0.699853\n",
      "[600]\ttraining's multi_logloss: 0.464678\tvalid_1's multi_logloss: 0.687138\n",
      "[900]\ttraining's multi_logloss: 0.425303\tvalid_1's multi_logloss: 0.686764\n",
      "Early stopping, best iteration is:\n",
      "[870]\ttraining's multi_logloss: 0.42893\tvalid_1's multi_logloss: 0.686173\n",
      "\n",
      "------------------------------ fold 7 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.529213\tvalid_1's multi_logloss: 0.620172\n",
      "[600]\ttraining's multi_logloss: 0.466847\tvalid_1's multi_logloss: 0.603175\n",
      "[900]\ttraining's multi_logloss: 0.425771\tvalid_1's multi_logloss: 0.601308\n",
      "Early stopping, best iteration is:\n",
      "[745]\ttraining's multi_logloss: 0.445598\tvalid_1's multi_logloss: 0.600841\n",
      "\n",
      "------------------------------ fold 8 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.527768\tvalid_1's multi_logloss: 0.64087\n",
      "[600]\ttraining's multi_logloss: 0.464901\tvalid_1's multi_logloss: 0.615592\n",
      "[900]\ttraining's multi_logloss: 0.424672\tvalid_1's multi_logloss: 0.612525\n",
      "[1200]\ttraining's multi_logloss: 0.392762\tvalid_1's multi_logloss: 0.613539\n",
      "Early stopping, best iteration is:\n",
      "[986]\ttraining's multi_logloss: 0.415095\tvalid_1's multi_logloss: 0.611997\n",
      "\n",
      "------------------------------ fold 9 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.524712\tvalid_1's multi_logloss: 0.626165\n"
     ]
    }
   ],
   "source": [
    "for pseudo_labeling_threshold in tqdm([0.95, 0.925, 0.9, 0.875, 0.85, -np.inf]):\n",
    "    df = df_main.copy()\n",
    "    \n",
    "    \n",
    "    # feature engineering\n",
    "    df[\"genre_name\"] = df[\"genre\"].map(dict(df_genre_labels[[\"labels\", \"genre\"]].values))\n",
    "    df[\"tempo\"] = df[\"tempo\"].map(lambda x: sum(map(int, x.split(\"-\"))) / 2)\n",
    "    df = pd.concat([df, pd.get_dummies(df[\"region\"]).rename(columns={\"unknown\": \"region_unknown\"})], axis=1)\n",
    "    df[\"num_nans\"] = 0\n",
    "    for col in [\n",
    "        \"acousticness\",\n",
    "        \"positiveness\",\n",
    "        \"danceability\",\n",
    "        \"energy\",\n",
    "        \"liveness\",\n",
    "        \"speechiness\",\n",
    "        \"instrumentalness\",\n",
    "    ]:\n",
    "        df[\"num_nans\"] += df[col].isna()\n",
    "    class CountEncoder:\n",
    "        def fit(self, series):\n",
    "            self.counts = series.groupby(series).count()\n",
    "            return self\n",
    "        def transform(self, series):\n",
    "            return series.map(self.counts).fillna(0)\n",
    "        def fit_transform(self, series):\n",
    "            return self.fit(series).transform(series)\n",
    "    columns_count_enc = [\"region\"]\n",
    "    for col in columns_count_enc:\n",
    "        df[\"countenc_\" + col] = CountEncoder().fit_transform(df[col])\n",
    "        df.loc[df[col].isna().values, \"countenc_\" + col] = np.nan\n",
    "    columns_label_enc = [\"region\"]\n",
    "    for col in columns_count_enc:\n",
    "        df[\"labelenc_\" + col] = LabelEncoder().fit_transform(df[col])\n",
    "        df.loc[df[col].isna().values, \"labelenc_\" + col] = np.nan\n",
    "  \n",
    "    df[\"log_tempo\"] = np.log(df[\"tempo\"])\n",
    "    gfe = GroupFeatureExtractor(\n",
    "        \"region\", \n",
    "        ['popularity', 'duration_ms', 'acousticness', 'positiveness', 'danceability', 'loudness', 'energy', 'liveness', 'speechiness', 'instrumentalness', 'log_tempo'],\n",
    "        [\"zscore\"]\n",
    "    )\n",
    "    df = pd.concat([df, gfe.fit_transform(df)], axis=1)\n",
    "\n",
    "    # feature scaling\n",
    "    df[\"log_tempo\"] = np.log(df[\"tempo\"])\n",
    "    for col in [\n",
    "        'popularity', 'duration_ms', 'acousticness',\n",
    "        'positiveness', 'danceability', 'loudness', 'energy', 'liveness',\n",
    "        'speechiness', 'instrumentalness', 'log_tempo', 'num_nans',\n",
    "    ]:\n",
    "        df[\"standardscaled_\" + col] = StandardScaler().fit_transform(df[[col]])[:, 0]\n",
    "    df_train, df_test = split_train_test(df)\n",
    "    target = df_train[\"genre\"]\n",
    "    \n",
    "    # train\n",
    "    \n",
    "    N_SPLITS = 15\n",
    "    SEED_SKF = 42\n",
    "    np.random.seed(42)\n",
    "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED_SKF)\n",
    "    oof = np.zeros((len(df_train), N_CLASSES))\n",
    "    predictions = np.zeros((len(df_test), N_CLASSES))\n",
    "    df_feature_importance = pd.DataFrame()\n",
    "    features_numerical = [\n",
    "        'popularity', 'duration_ms', 'acousticness',\n",
    "        'positiveness', 'danceability', 'loudness', 'energy', 'liveness',\n",
    "        'speechiness', 'instrumentalness', 'tempo',\n",
    "        'region_A', 'region_B', 'region_C', 'region_D', 'region_E', 'region_F',\n",
    "        'region_G', 'region_H', 'region_I', 'region_J', 'region_K', 'region_L',\n",
    "        'region_M', 'region_N', 'region_O', 'region_P', 'region_Q', 'region_R',\n",
    "        'region_S', 'region_T', 'region_unknown', 'countenc_region',\n",
    "        'num_nans',\n",
    "        'agg_zscore_popularity_grpby_region',\n",
    "        'agg_zscore_duration_ms_grpby_region',\n",
    "        'agg_zscore_acousticness_grpby_region',\n",
    "        'agg_zscore_positiveness_grpby_region',\n",
    "        'agg_zscore_danceability_grpby_region',\n",
    "        'agg_zscore_loudness_grpby_region', 'agg_zscore_energy_grpby_region',\n",
    "        'agg_zscore_liveness_grpby_region',\n",
    "        'agg_zscore_speechiness_grpby_region',\n",
    "        'agg_zscore_instrumentalness_grpby_region',\n",
    "        'agg_zscore_log_tempo_grpby_region',\n",
    "        'knn_score_class00', 'knn_score_class01',\n",
    "        'knn_score_class02', 'knn_score_class03', 'knn_score_class04',\n",
    "        'knn_score_class05', 'knn_score_class06', 'knn_score_class07',\n",
    "        'knn_score_class08', 'knn_score_class09', 'knn_score_class10',\n",
    "        'max_knn_scores',\n",
    "        'sub_max_knn_scores_knn_score_class00',\n",
    "        'sub_max_knn_scores_knn_score_class01',\n",
    "        'sub_max_knn_scores_knn_score_class02',\n",
    "        'sub_max_knn_scores_knn_score_class03',\n",
    "        'sub_max_knn_scores_knn_score_class04',\n",
    "        'sub_max_knn_scores_knn_score_class05',\n",
    "        'sub_max_knn_scores_knn_score_class06',\n",
    "        'sub_max_knn_scores_knn_score_class07',\n",
    "        'sub_max_knn_scores_knn_score_class08',\n",
    "        'sub_max_knn_scores_knn_score_class09',\n",
    "        'sub_max_knn_scores_knn_score_class10',\n",
    "        'sum_knn_scores'\n",
    "    ]\n",
    "    sub_knn_score_n_m = []\n",
    "    score_columns = [f\"knn_score_class{c:02d}\" for c in range(N_CLASSES)]\n",
    "    for i, col1 in enumerate(score_columns):\n",
    "        for j, col2 in enumerate(score_columns[i+1:], i+1):\n",
    "            sub_knn_score_n_m.append(f'sub_{col1}_{col2}')\n",
    "            \n",
    "    features_numerical += sub_knn_score_n_m        \n",
    "    features_categorical = [\"labelenc_region\"]\n",
    "    features = features_numerical + features_categorical\n",
    "    for fold_, (indexes_trn, indexes_val) in enumerate(skf.split(df_train.values, target.values)):\n",
    "        print(f\"------------------------------ fold {fold_} ------------------------------\")\n",
    "        df_trn = df_train.loc[indexes_trn].reset_index(drop=True)\n",
    "        df_val = df_train.loc[indexes_val].reset_index(drop=True)\n",
    "        target_trn = target.loc[indexes_trn].reset_index(drop=True)\n",
    "        target_val = target.loc[indexes_val].reset_index(drop=True)\n",
    "        # make knn features\n",
    "        X = df_trn[knn_features].fillna(0.0).values * knn_feature_weights\n",
    "        knn_feature_extractor = KNNFeatureExtractor(knn_n_neighbors).fit(X, target_trn)\n",
    "        df_trn = pd.concat([df_trn, knn_feature_extractor.transform(X, is_train_data=True)], axis=1)\n",
    "        X = df_val[knn_features].fillna(0.0).values * knn_feature_weights\n",
    "        df_val = pd.concat([df_val, knn_feature_extractor.transform(X, is_train_data=False)], axis=1)\n",
    "        X = df_test[knn_features].fillna(0.0).values * knn_feature_weights\n",
    "        df_test_knn_features = knn_feature_extractor.transform(X, is_train_data=False)\n",
    "        for col in df_test_knn_features.columns:\n",
    "            df_test[col] = df_test_knn_features[col]\n",
    "        lgb_train = lgb.Dataset(\n",
    "            df_trn.loc[:, features],\n",
    "            label=target_trn,\n",
    "            feature_name=features,\n",
    "            categorical_feature=features_categorical\n",
    "        )\n",
    "        lgb_valid = lgb.Dataset(\n",
    "            df_val.loc[:, features],\n",
    "            label=target_val,\n",
    "            feature_name=features,\n",
    "            categorical_feature=features_categorical\n",
    "        )\n",
    "        lgb_params[\"learning_rate\"] = learning_rate + np.random.random() * 0.001  # おまじない\n",
    "        num_round = 999999999\n",
    "        model = lgb.train(\n",
    "            lgb_params,\n",
    "            lgb_train, \n",
    "            num_round, \n",
    "            valid_sets=[lgb_train, lgb_valid], \n",
    "            verbose_eval=300,\n",
    "            early_stopping_rounds=300 if num_round >= 1e8 else None,\n",
    "            fobj=None,\n",
    "            #feval=lgb_metric,\n",
    "        )\n",
    "        # cv\n",
    "        prediction_round = model.best_iteration+150 if num_round >= 1e8 else num_round  # おまじない\n",
    "        oof[indexes_val] = model.predict(df_val[features], num_iteration=prediction_round)\n",
    "        # feature importance\n",
    "        df_fold_importance = pd.DataFrame()\n",
    "        df_fold_importance[\"feature\"] = features\n",
    "        df_fold_importance[\"importance\"] = model.feature_importance()\n",
    "        df_fold_importance[\"fold\"] = fold_\n",
    "        df_feature_importance = pd.concat([df_feature_importance, df_fold_importance], axis=0)\n",
    "        # prediction for test data\n",
    "        predictions += model.predict(df_test[features], num_iteration=prediction_round) / N_SPLITS\n",
    "        print()\n",
    "    \n",
    "    score = f1_score(target, oof.argmax(1), average=\"macro\")\n",
    "    print(\"CV score (not reliable!)\")\n",
    "    print(f\"  f1: {score:8.5f}\")\n",
    "    print()\n",
    "    print(classification_report(target, oof.argmax(1)))\n",
    "    \n",
    "    df_test[\"prediction\"] = predictions.argmax(1)\n",
    "    df_test[\"confidence\"] = predictions.max(1)\n",
    "    df_test[\"genre\"] = np.where(predictions.max(1) > pseudo_labeling_threshold, predictions.argmax(1), -100)\n",
    "    df = merge_train_test(df_train, df_test)\n",
    "    df_main[\"genre\"] = df_main[\"index\"].map(dict(df[[\"index\", \"genre\"]].values))\n",
    "    print((df_test[\"confidence\"] > pseudo_labeling_threshold).sum(), f\"rows were filled. (confidence>{pseudo_labeling_threshold})\")\n",
    "    print(\"filled test labels:\", np.bincount(df_test[df_test[\"genre\"]!=-100][\"genre\"]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d997dd70-0030-4f8d-885a-8c0ea84b40a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = df_sample_sub.copy()\n",
    "df_submission[\"genre\"] = df_submission[\"index\"].map(dict(df_main[[\"index\", \"genre\"]].values))\n",
    "# genreのどこにもnanがなかったらという条件。あるならAssertion Error\n",
    "assert not df_submission[\"genre\"].isna().any()\n",
    "\n",
    "print(\"genre counts\")\n",
    "display(df_submission[\"genre\"].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nfirst 10 test data\")\n",
    "display(df_submission.head(10))\n",
    "\n",
    "# make submission file\n",
    "df_submission.to_csv(\"../outputs/sub_05_06.csv\", header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb200a05-f51b-4496-8713-2ade203dc9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_confusion_matrix(y_true,\n",
    "                                                      pred_label,\n",
    "                                                      height=.6,\n",
    "                                                      labels=None):\n",
    "    conf = confusion_matrix(y_true=y_true,\n",
    "                                               y_pred=pred_label,\n",
    "                                               normalize='true')\n",
    "    \n",
    "    n_labels = len(conf)\n",
    "    size = n_labels * height\n",
    "    fig , ax = plt.subplots(figsize=(size*2, size*1.5))\n",
    "    sns.heatmap(conf, cmap='YlOrBr', ax=ax, annot=True, fmt='.2f')\n",
    "    ax.set_ylabel('Label')\n",
    "    ax.set_xlabel('Predict')\n",
    "    \n",
    "    if labels is not None:\n",
    "        ax.set_yticklabels(labels)\n",
    "        ax.set_xticklabels(labels)\n",
    "        ax.tick_params('y', labelrotation=0)\n",
    "        ax.tick_params('x', labelrotation=90)\n",
    "        \n",
    "    return fig\n",
    "\n",
    "oof_label = pd.Series(np.argmax(oof, axis=1))\n",
    "fig = visualize_confusion_matrix(y_true=target, pred_label=oof_label, labels=df_genre_labels['genre'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2005f437-a0be-4401-b95a-91534957a18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adversal Validation\n",
    "from Adversal_Validation import Lgbm\n",
    "\n",
    "df_tn = df_train.copy()\n",
    "df_tt = df_test.copy()\n",
    "\n",
    "X = df_tn[knn_features].fillna(0.0).values * knn_feature_weights\n",
    "    \n",
    "knn_feature_extractor = KNNFeatureExtractor(knn_n_neighbors).fit(X, target)\n",
    "df_tn = pd.concat([df_tn, knn_feature_extractor.transform(X, is_train_data=True)], axis=1)\n",
    "    \n",
    "X = df_test[knn_features].fillna(0.0).values * knn_feature_weights\n",
    "df_test_knn_features = knn_feature_extractor.transform(X, is_train_data=False)\n",
    "\n",
    "train_feat_df = df_tn.drop(['genre', 'index', 'region', 'genre_name'], axis=1)\n",
    "test_feat_df = df_tt.drop(['genre', 'index', 'region', 'genre_name', 'prediction'], axis=1)\n",
    "\n",
    "\n",
    "params = {'boosting_type': 'gbdt',\n",
    "          'objective': 'cross_entropy',\n",
    "          'verbosity':-1}\n",
    "\n",
    "model_ad = Lgbm(params)\n",
    "model_ad.adversal_validation(train_feat_df,test_feat_df)\n",
    "model_ad.visualize_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711c1d9a-b0bd-4a5d-bd55-f7ae616261b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_sub['genre'] = predictions.argmax(axis=1)\n",
    "display(df_sample_sub)\n",
    "df_sample_sub.to_csv('../outputs/sub_3.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081db819-c893-4983-af06-1ca1a778e3fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
