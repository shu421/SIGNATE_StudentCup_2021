{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25edb1c3-2e18-4297-b568-d2d73825d645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/shugo/Desktop/SIGNATE/SIGNATE_StudentCup2021/shu421/notebooks', '/Library/Frameworks/Python.framework/Versions/3.7/lib/python37.zip', '/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7', '/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/lib-dynload', '', '/Users/shugo/Desktop/SIGNATE/SIGNATE_StudentCup2021/shu421/lib/python3.7/site-packages', '/Users/shugo/Desktop/SIGNATE/SIGNATE_StudentCup2021/shu421/lib/python3.7/site-packages/IPython/extensions', '/Users/shugo/.ipython', '../modules/']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import sys\n",
    "print(sys.path)\n",
    "sys.path.append('../modules/')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_origin = pd.read_csv('../input/train.csv')\n",
    "test_origin = pd.read_csv('../input/test.csv')\n",
    "\n",
    "df_train = train_origin.copy()\n",
    "df_test = test_origin.copy()\n",
    "\n",
    "df_sample_sub = pd.read_csv('../input/sample_submit.csv', header=None)\n",
    "df_sample_sub.columns = ['index', 'genre']\n",
    "df_genre_labels = pd.read_csv('../input/genre_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1eef89a-ea0d-4212-ab5b-141fb9b269e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = 11\n",
    "\n",
    "# testのジャンルを-100として結合\n",
    "def merge_train_test(df_train, df_test):\n",
    "    if 'genre' not in df_test.columns.tolist():\n",
    "        df_test['genre'] = -100\n",
    "    res = pd.concat([df_train, df_test])\n",
    "    res.reset_index(inplace=True, drop = True)\n",
    "    return res\n",
    "\n",
    "def split_train_test(df):\n",
    "    df_train = df[df['genre'] != -100]\n",
    "    df_test = df[df['genre'] == -100]\n",
    "    df_train.reset_index(inplace=True, drop=True)\n",
    "    df_test.reset_index(inplace=True, drop =True)\n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "\n",
    "warnings.simplefilter('ignore', pd.core.common.SettingWithCopyWarning)\n",
    "warnings.simplefilter('ignore', UserWarning)\n",
    "\n",
    "\n",
    "N_CLASSES = 11\n",
    "\n",
    "\n",
    "INPUT = Path(\"../input\")\n",
    "df_train = pd.read_csv(INPUT / \"train.csv\")\n",
    "df_test = pd.read_csv(INPUT / \"test.csv\")\n",
    "df_sample_sub = pd.read_csv(INPUT / \"sample_submit.csv\", header=None)\n",
    "df_sample_sub.columns = [\"index\", \"genre\"]\n",
    "df_genre_labels = pd.read_csv(INPUT / \"genre_labels.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# parameters\n",
    "\n",
    "# def lgb_metric(preds, data):  \n",
    "#     pred_labels = preds.reshape(N_CLASSES, -1).argmax(axis=0)\n",
    "#     score = f1_score(data.get_label(), pred_labels, average=\"macro\")\n",
    "#     return \"macro_f1\", score, True\n",
    "\n",
    "learning_rate = 0.01\n",
    "lgb_params = {\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"num_class\": N_CLASSES,\n",
    "    #\"metric\": \"None\",\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"num_leaves\": 3,\n",
    "    \"min_data_in_leaf\": 40,\n",
    "    #\"colsample_bytree\": 1.0,\n",
    "    #\"feature_fraction\": 1.0,\n",
    "    #\"bagging_freq\": 0,\n",
    "    #\"bagging_fraction\": 1.0,\n",
    "    \"verbosity\": 0,\n",
    "    \"seed\": 42,\n",
    "    \"force_col_wise\":True\n",
    "}\n",
    "\n",
    "knn_n_neighbors = 6\n",
    "\n",
    "\n",
    "# parameters - knn feature weights\n",
    "\n",
    "knn_features = [\n",
    "   'region_A', 'region_B', 'region_C', 'region_D', 'region_E', 'region_F',\n",
    "   'region_G', 'region_H', 'region_I', 'region_J', 'region_K', 'region_L',\n",
    "   'region_M', 'region_N', 'region_O', 'region_P', 'region_Q', 'region_R',\n",
    "   'region_S', 'region_T', 'region_unknown',\n",
    "   'standardscaled_popularity', 'standardscaled_duration_ms',\n",
    "   'standardscaled_acousticness', 'standardscaled_positiveness',\n",
    "   'standardscaled_danceability', 'standardscaled_loudness',\n",
    "   'standardscaled_energy', 'standardscaled_liveness',\n",
    "   'standardscaled_speechiness', 'standardscaled_instrumentalness',\n",
    "   'standardscaled_log_tempo', 'standardscaled_num_nans'\n",
    "]\n",
    "\n",
    "dict_feature_weights = {}\n",
    "\n",
    "for col in [\n",
    "    'region_A', 'region_B', 'region_C', 'region_D', 'region_E', 'region_F',\n",
    "    'region_G', 'region_H', 'region_I', 'region_J', 'region_K', 'region_L',\n",
    "    'region_M', 'region_N', 'region_O', 'region_P', 'region_Q', 'region_R',\n",
    "    'region_S', 'region_T', 'region_unknown'\n",
    "]:\n",
    "    dict_feature_weights[col] = 100.0\n",
    "\n",
    "for col in [\n",
    "    'standardscaled_duration_ms',\n",
    "    'standardscaled_acousticness', 'standardscaled_positiveness',\n",
    "    'standardscaled_danceability', 'standardscaled_loudness',\n",
    "    'standardscaled_energy', 'standardscaled_liveness',\n",
    "    'standardscaled_speechiness', 'standardscaled_instrumentalness'\n",
    "]:\n",
    "    dict_feature_weights[col] = 1.0\n",
    "\n",
    "dict_feature_weights[\"standardscaled_popularity\"] = 8.0\n",
    "dict_feature_weights[\"standardscaled_log_tempo\"] = 0.001\n",
    "dict_feature_weights[\"standardscaled_num_nans\"] = 100.0\n",
    "\n",
    "knn_feature_weights = np.array([dict_feature_weights[col] for col in knn_features])\n",
    "\n",
    "def merge_train_test(df_train, df_test):\n",
    "\n",
    "    if \"genre\" not in df_test.columns.tolist():\n",
    "        df_test[\"genre\"] = -100\n",
    "    res = pd.concat([df_train, df_test])\n",
    "    res.reset_index(inplace=True, drop=True)\n",
    "    return res\n",
    "\n",
    "def split_train_test(df):\n",
    "    df_train = df[df[\"genre\"] != -100]\n",
    "    df_test = df[df[\"genre\"] == -100]\n",
    "    df_train.reset_index(inplace=True, drop=True)\n",
    "    df_test.reset_index(inplace=True, drop=True)\n",
    "    return df_train, df_test\n",
    "\n",
    "df = merge_train_test(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc67f9ca-ae6f-4faa-b612-0407c3516d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ fold 0 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.734242\tvalid_1's multi_logloss: 0.899019\n",
      "[600]\ttraining's multi_logloss: 0.653358\tvalid_1's multi_logloss: 0.868332\n",
      "[900]\ttraining's multi_logloss: 0.600308\tvalid_1's multi_logloss: 0.865146\n",
      "[1200]\ttraining's multi_logloss: 0.556603\tvalid_1's multi_logloss: 0.86466\n",
      "[1500]\ttraining's multi_logloss: 0.51985\tvalid_1's multi_logloss: 0.863897\n",
      "Early stopping, best iteration is:\n",
      "[1378]\ttraining's multi_logloss: 0.534294\tvalid_1's multi_logloss: 0.863008\n",
      "\n",
      "------------------------------ fold 1 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.735646\tvalid_1's multi_logloss: 0.794617\n",
      "[600]\ttraining's multi_logloss: 0.651592\tvalid_1's multi_logloss: 0.778057\n",
      "[900]\ttraining's multi_logloss: 0.595513\tvalid_1's multi_logloss: 0.772955\n",
      "[1200]\ttraining's multi_logloss: 0.549959\tvalid_1's multi_logloss: 0.775998\n",
      "Early stopping, best iteration is:\n",
      "[1019]\ttraining's multi_logloss: 0.576445\tvalid_1's multi_logloss: 0.772116\n",
      "\n",
      "------------------------------ fold 2 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.733331\tvalid_1's multi_logloss: 0.821272\n",
      "[600]\ttraining's multi_logloss: 0.649972\tvalid_1's multi_logloss: 0.806224\n",
      "[900]\ttraining's multi_logloss: 0.594071\tvalid_1's multi_logloss: 0.807136\n",
      "Early stopping, best iteration is:\n",
      "[707]\ttraining's multi_logloss: 0.628164\tvalid_1's multi_logloss: 0.805056\n",
      "\n",
      "------------------------------ fold 3 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.74779\tvalid_1's multi_logloss: 0.753604\n",
      "[600]\ttraining's multi_logloss: 0.663856\tvalid_1's multi_logloss: 0.734393\n",
      "[900]\ttraining's multi_logloss: 0.608721\tvalid_1's multi_logloss: 0.733824\n",
      "Early stopping, best iteration is:\n",
      "[670]\ttraining's multi_logloss: 0.649366\tvalid_1's multi_logloss: 0.733258\n",
      "\n",
      "------------------------------ fold 4 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.745451\tvalid_1's multi_logloss: 0.799897\n",
      "[600]\ttraining's multi_logloss: 0.662805\tvalid_1's multi_logloss: 0.770528\n",
      "[900]\ttraining's multi_logloss: 0.607384\tvalid_1's multi_logloss: 0.765206\n",
      "[1200]\ttraining's multi_logloss: 0.563559\tvalid_1's multi_logloss: 0.764489\n",
      "[1500]\ttraining's multi_logloss: 0.526823\tvalid_1's multi_logloss: 0.764709\n",
      "Early stopping, best iteration is:\n",
      "[1290]\ttraining's multi_logloss: 0.552\tvalid_1's multi_logloss: 0.763512\n",
      "\n",
      "------------------------------ fold 5 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.742535\tvalid_1's multi_logloss: 0.84084\n",
      "[600]\ttraining's multi_logloss: 0.658257\tvalid_1's multi_logloss: 0.827818\n",
      "[900]\ttraining's multi_logloss: 0.602203\tvalid_1's multi_logloss: 0.829785\n",
      "Early stopping, best iteration is:\n",
      "[612]\ttraining's multi_logloss: 0.655591\tvalid_1's multi_logloss: 0.827598\n",
      "\n",
      "------------------------------ fold 6 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.752175\tvalid_1's multi_logloss: 0.730111\n",
      "[600]\ttraining's multi_logloss: 0.669363\tvalid_1's multi_logloss: 0.698288\n",
      "[900]\ttraining's multi_logloss: 0.61434\tvalid_1's multi_logloss: 0.6951\n",
      "[1200]\ttraining's multi_logloss: 0.570113\tvalid_1's multi_logloss: 0.694609\n",
      "Early stopping, best iteration is:\n",
      "[1117]\ttraining's multi_logloss: 0.581499\tvalid_1's multi_logloss: 0.694252\n",
      "\n",
      "------------------------------ fold 7 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.72813\tvalid_1's multi_logloss: 0.860451\n",
      "[600]\ttraining's multi_logloss: 0.644917\tvalid_1's multi_logloss: 0.850061\n",
      "Early stopping, best iteration is:\n",
      "[492]\ttraining's multi_logloss: 0.669736\tvalid_1's multi_logloss: 0.848437\n",
      "\n",
      "------------------------------ fold 8 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.736872\tvalid_1's multi_logloss: 0.911019\n",
      "[600]\ttraining's multi_logloss: 0.652432\tvalid_1's multi_logloss: 0.889484\n",
      "[900]\ttraining's multi_logloss: 0.596771\tvalid_1's multi_logloss: 0.883944\n",
      "[1200]\ttraining's multi_logloss: 0.553192\tvalid_1's multi_logloss: 0.883115\n",
      "[1500]\ttraining's multi_logloss: 0.516797\tvalid_1's multi_logloss: 0.884192\n",
      "Early stopping, best iteration is:\n",
      "[1218]\ttraining's multi_logloss: 0.550786\tvalid_1's multi_logloss: 0.882779\n",
      "\n",
      "------------------------------ fold 9 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.731761\tvalid_1's multi_logloss: 0.91087\n",
      "[600]\ttraining's multi_logloss: 0.64902\tvalid_1's multi_logloss: 0.873749\n",
      "[900]\ttraining's multi_logloss: 0.593998\tvalid_1's multi_logloss: 0.864088\n",
      "[1200]\ttraining's multi_logloss: 0.549798\tvalid_1's multi_logloss: 0.860696\n",
      "[1500]\ttraining's multi_logloss: 0.512076\tvalid_1's multi_logloss: 0.859697\n",
      "[1800]\ttraining's multi_logloss: 0.478442\tvalid_1's multi_logloss: 0.860568\n",
      "Early stopping, best iteration is:\n",
      "[1636]\ttraining's multi_logloss: 0.496487\tvalid_1's multi_logloss: 0.858463\n",
      "\n",
      "------------------------------ fold 10 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.745867\tvalid_1's multi_logloss: 0.808618\n",
      "[600]\ttraining's multi_logloss: 0.663342\tvalid_1's multi_logloss: 0.791166\n",
      "Early stopping, best iteration is:\n",
      "[466]\ttraining's multi_logloss: 0.693922\tvalid_1's multi_logloss: 0.788987\n",
      "\n",
      "------------------------------ fold 11 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.731675\tvalid_1's multi_logloss: 0.809442\n",
      "[600]\ttraining's multi_logloss: 0.648234\tvalid_1's multi_logloss: 0.791469\n",
      "[900]\ttraining's multi_logloss: 0.592789\tvalid_1's multi_logloss: 0.78707\n",
      "[1200]\ttraining's multi_logloss: 0.546503\tvalid_1's multi_logloss: 0.789845\n",
      "Early stopping, best iteration is:\n",
      "[991]\ttraining's multi_logloss: 0.577859\tvalid_1's multi_logloss: 0.786366\n",
      "\n",
      "------------------------------ fold 12 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.728579\tvalid_1's multi_logloss: 0.914752\n",
      "[600]\ttraining's multi_logloss: 0.644211\tvalid_1's multi_logloss: 0.883684\n",
      "[900]\ttraining's multi_logloss: 0.588954\tvalid_1's multi_logloss: 0.884256\n",
      "Early stopping, best iteration is:\n",
      "[734]\ttraining's multi_logloss: 0.617364\tvalid_1's multi_logloss: 0.883467\n",
      "\n",
      "------------------------------ fold 13 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.734167\tvalid_1's multi_logloss: 0.861599\n",
      "[600]\ttraining's multi_logloss: 0.653069\tvalid_1's multi_logloss: 0.838018\n",
      "[900]\ttraining's multi_logloss: 0.599714\tvalid_1's multi_logloss: 0.834818\n",
      "Early stopping, best iteration is:\n",
      "[891]\ttraining's multi_logloss: 0.601144\tvalid_1's multi_logloss: 0.834436\n",
      "\n",
      "------------------------------ fold 14 ------------------------------\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[300]\ttraining's multi_logloss: 0.744335\tvalid_1's multi_logloss: 0.816614\n",
      "[600]\ttraining's multi_logloss: 0.66018\tvalid_1's multi_logloss: 0.80271\n",
      "Early stopping, best iteration is:\n",
      "[586]\ttraining's multi_logloss: 0.663247\tvalid_1's multi_logloss: 0.802394\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# feature engineering\n",
    "df[\"genre_name\"] = df[\"genre\"].map(dict(df_genre_labels[[\"labels\", \"genre\"]].values))\n",
    "df[\"tempo\"] = df[\"tempo\"].map(lambda x: sum(map(int, x.split(\"-\"))) / 2)\n",
    "df = pd.concat([df, pd.get_dummies(df[\"region\"]).rename(columns={\"unknown\": \"region_unknown\"})], axis=1)\n",
    "df[\"num_nans\"] = 0\n",
    "for col in [\n",
    "    \"acousticness\",\n",
    "    \"positiveness\",\n",
    "    \"danceability\",\n",
    "    \"energy\",\n",
    "    \"liveness\",\n",
    "    \"speechiness\",\n",
    "    \"instrumentalness\",\n",
    "]:\n",
    "    df[\"num_nans\"] += df[col].isna()\n",
    "class CountEncoder:\n",
    "    def fit(self, series):\n",
    "        self.counts = series.groupby(series).count()\n",
    "        return self\n",
    "    def transform(self, series):\n",
    "        return series.map(self.counts).fillna(0)\n",
    "    def fit_transform(self, series):\n",
    "        return self.fit(series).transform(series)\n",
    "columns_count_enc = [\"region\"]\n",
    "for col in columns_count_enc:\n",
    "    df[\"countenc_\" + col] = CountEncoder().fit_transform(df[col])\n",
    "    df.loc[df[col].isna().values, \"countenc_\" + col] = np.nan\n",
    "columns_label_enc = [\"region\"]\n",
    "for col in columns_count_enc:\n",
    "    df[\"labelenc_\" + col] = LabelEncoder().fit_transform(df[col])\n",
    "    df.loc[df[col].isna().values, \"labelenc_\" + col] = np.nan\n",
    "class GroupFeatureExtractor:  # 参考: https://signate.jp/competitions/449/discussions/lgbm-baseline-lb06240\n",
    "    EX_TRANS_METHODS = [\"deviation\", \"zscore\"]\n",
    "    def __init__(self, group_key, group_values, agg_methods):\n",
    "        self.group_key = group_key\n",
    "        self.group_values = group_values\n",
    "        self.ex_trans_methods = [m for m in agg_methods if m in self.EX_TRANS_METHODS]\n",
    "        self.agg_methods = [m for m in agg_methods if m not in self.ex_trans_methods]\n",
    "        self.df_agg = None\n",
    "    def fit(self, df_train, y=None):\n",
    "        if not self.agg_methods:\n",
    "            return\n",
    "        dfs = []\n",
    "        for agg_method in self.agg_methods:\n",
    "            if callable(agg_method):\n",
    "                agg_method_name = agg_method.__name__\n",
    "            else:\n",
    "                agg_method_name = agg_method\n",
    "            df_agg = (df_train[[self.group_key] + self.group_values].groupby(self.group_key).agg(agg_method))\n",
    "            df_agg.columns = self._get_column_names(agg_method_name)\n",
    "            dfs.append(df_agg)\n",
    "        self.df_agg = pd.concat(dfs, axis=1).reset_index()\n",
    "    def transform(self, df_eval):\n",
    "        key = self.group_key\n",
    "        if self.agg_methods:\n",
    "            df_features = pd.merge(df_eval[[self.group_key]], self.df_agg, on=self.group_key, how=\"left\")\n",
    "        else:\n",
    "            df_features = df_eval[[self.group_key]].copy()\n",
    "        if self.ex_trans_methods:\n",
    "            if \"deviation\" in self.ex_trans_methods:\n",
    "                df_features[self._get_agg_column_names(\"deviation\")] = df_eval[self.group_values] - df_eval[[key]+self.group_values].groupby(key).transform(\"mean\")\n",
    "            if \"zscore\" in self.ex_trans_methods:\n",
    "                df_features[self._get_column_names(\"zscore\")] = (df_eval[self.group_values] - df_eval[[key]+self.group_values].groupby(key).transform(\"mean\")) \\\n",
    "                                                                / (df_eval[[key]+self.group_values].groupby(key).transform(\"std\") + 1e-8)\n",
    "        df_features.drop(self.group_key, axis=1, inplace=True)\n",
    "        return df_features\n",
    "    def _get_column_names(self, method):\n",
    "        return [f\"agg_{method}_{col}_grpby_{self.group_key}\" for col in self.group_values]\n",
    "    def fit_transform(self, df_train, y=None):\n",
    "        self.fit(df_train, y=y)\n",
    "        return self.transform(df_train)   \n",
    "df[\"log_tempo\"] = np.log(df[\"tempo\"])\n",
    "gfe = GroupFeatureExtractor(\n",
    "    \"region\", \n",
    "    ['popularity', 'duration_ms', 'acousticness', 'positiveness', 'danceability', 'loudness', 'energy', 'liveness', 'speechiness', 'instrumentalness', 'log_tempo'],\n",
    "    [\"zscore\"]\n",
    ")\n",
    "df = pd.concat([df, gfe.fit_transform(df)], axis=1)\n",
    "class KNNFeatureExtractor:\n",
    "    def __init__(self, n_neighbors=5):\n",
    "        self.knn = KNeighborsClassifier(n_neighbors + 1)\n",
    "    def fit(self, X, y):\n",
    "        self.knn.fit(X, y)\n",
    "        self.y = y if isinstance(y, np.ndarray) else np.array(y)\n",
    "        return self\n",
    "    def transform(self, X, is_train_data):\n",
    "        distances, indexes = self.knn.kneighbors(X)\n",
    "        distances = distances[:, 1:] if is_train_data else distances[:, :-1]\n",
    "        indexes = indexes[:, 1:] if is_train_data else indexes[:, :-1]\n",
    "        labels = self.y[indexes]\n",
    "        score_columns = [f\"knn_score_class{c:02d}\" for c in range(N_CLASSES)]\n",
    "        df_knn = pd.DataFrame(\n",
    "            [np.bincount(labels_, distances_, N_CLASSES) for labels_, distances_ in zip(labels, 1.0 / distances)],\n",
    "            columns=score_columns\n",
    "        )\n",
    "        # 最大スコア\n",
    "        df_knn['max_knn_scores'] = df_knn.max(axis=1)\n",
    "\n",
    "        # 最大スコアとの差。0は最大スコアを表す\n",
    "        for col in score_columns:\n",
    "            df_knn[f'sub_max_knn_scores_{col}'] = df_knn['max_knn_scores'] - df_knn[col]\n",
    "\n",
    "        # 最大スコアとの比。1は最大スコアを表す\n",
    "        for col in score_columns:\n",
    "            df_knn[f'div_max_knn_scores_{col}'] = df_knn[col] / df_knn['max_knn_scores']\n",
    "\n",
    "        # それぞれのスコア同士の差\n",
    "        for i, col1 in enumerate(score_columns):\n",
    "            for j, col2 in enumerate(score_columns[i+1:], i+1): # 全パターンを網羅できる\n",
    "                df_knn[f'sub_{col1}_{col2}'] = df_knn[col1] - df_knn[col2]\n",
    "\n",
    "        # knnスコアの合計\n",
    "        df_knn['sum_knn_scores'] = df_knn.sum(axis=1)\n",
    "\n",
    "        return df_knn\n",
    "# feature scaling\n",
    "df[\"log_tempo\"] = np.log(df[\"tempo\"])\n",
    "for col in [\n",
    "    'popularity', 'duration_ms', 'acousticness',\n",
    "    'positiveness', 'danceability', 'loudness', 'energy', 'liveness',\n",
    "    'speechiness', 'instrumentalness', 'log_tempo', 'num_nans',\n",
    "]:\n",
    "    df[\"standardscaled_\" + col] = StandardScaler().fit_transform(df[[col]])[:, 0]\n",
    "df_train, df_test = split_train_test(df)\n",
    "target = df_train[\"genre\"]\n",
    "\n",
    "\n",
    "# train\n",
    "\n",
    "N_SPLITS = 15\n",
    "SEED_SKF = 42\n",
    "np.random.seed(42)\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED_SKF)\n",
    "oof = np.zeros((len(df_train), N_CLASSES))\n",
    "predictions = np.zeros((len(df_test), N_CLASSES))\n",
    "df_feature_importance = pd.DataFrame()\n",
    "features_numerical = [\n",
    "    'popularity', 'duration_ms', 'acousticness',\n",
    "    'positiveness', 'danceability', 'loudness', 'energy', 'liveness',\n",
    "    'speechiness', 'instrumentalness', 'tempo',\n",
    "    'region_A', 'region_B', 'region_C', 'region_D', 'region_E', 'region_F',\n",
    "    'region_G', 'region_H', 'region_I', 'region_J', 'region_K', 'region_L',\n",
    "    'region_M', 'region_N', 'region_O', 'region_P', 'region_Q', 'region_R',\n",
    "    'region_S', 'region_T', 'region_unknown', 'countenc_region',\n",
    "    'num_nans',\n",
    "    'agg_zscore_popularity_grpby_region',\n",
    "    'agg_zscore_duration_ms_grpby_region',\n",
    "    'agg_zscore_acousticness_grpby_region',\n",
    "    'agg_zscore_positiveness_grpby_region',\n",
    "    'agg_zscore_danceability_grpby_region',\n",
    "    'agg_zscore_loudness_grpby_region', 'agg_zscore_energy_grpby_region',\n",
    "    'agg_zscore_liveness_grpby_region',\n",
    "    'agg_zscore_speechiness_grpby_region',\n",
    "    'agg_zscore_instrumentalness_grpby_region',\n",
    "    'agg_zscore_log_tempo_grpby_region',\n",
    "    'knn_score_class00', 'knn_score_class01',\n",
    "    'knn_score_class02', 'knn_score_class03', 'knn_score_class04',\n",
    "    'knn_score_class05', 'knn_score_class06', 'knn_score_class07',\n",
    "    'knn_score_class08', 'knn_score_class09', 'knn_score_class10',\n",
    "    'max_knn_scores',\n",
    "    'sub_max_knn_scores_knn_score_class00',\n",
    "    'sub_max_knn_scores_knn_score_class01',\n",
    "    'sub_max_knn_scores_knn_score_class02',\n",
    "    'sub_max_knn_scores_knn_score_class03',\n",
    "    'sub_max_knn_scores_knn_score_class04',\n",
    "    'sub_max_knn_scores_knn_score_class05',\n",
    "    'sub_max_knn_scores_knn_score_class06',\n",
    "    'sub_max_knn_scores_knn_score_class07',\n",
    "    'sub_max_knn_scores_knn_score_class08',\n",
    "    'sub_max_knn_scores_knn_score_class09',\n",
    "    'sub_max_knn_scores_knn_score_class10',\n",
    "    'sum_knn_scores'\n",
    "]\n",
    "sub_knn_score_n_m = []\n",
    "score_columns = [f\"knn_score_class{c:02d}\" for c in range(N_CLASSES)]\n",
    "for i, col1 in enumerate(score_columns):\n",
    "    for j, col2 in enumerate(score_columns[i+1:], i+1):\n",
    "        sub_knn_score_n_m.append(f'sub_{col1}_{col2}')\n",
    "\n",
    "features_numerical += sub_knn_score_n_m        \n",
    "features_categorical = [\"labelenc_region\"]\n",
    "features = features_numerical + features_categorical\n",
    "for fold_, (indexes_trn, indexes_val) in enumerate(skf.split(df_train.values, target.values)):\n",
    "    print(f\"------------------------------ fold {fold_} ------------------------------\")\n",
    "    df_trn = df_train.loc[indexes_trn].reset_index(drop=True)\n",
    "    df_val = df_train.loc[indexes_val].reset_index(drop=True)\n",
    "    target_trn = target.loc[indexes_trn].reset_index(drop=True)\n",
    "    target_val = target.loc[indexes_val].reset_index(drop=True)\n",
    "    # make knn features\n",
    "    X = df_trn[knn_features].fillna(0.0).values * knn_feature_weights\n",
    "    knn_feature_extractor = KNNFeatureExtractor(knn_n_neighbors).fit(X, target_trn)\n",
    "    df_trn = pd.concat([df_trn, knn_feature_extractor.transform(X, is_train_data=True)], axis=1)\n",
    "    X = df_val[knn_features].fillna(0.0).values * knn_feature_weights\n",
    "    df_val = pd.concat([df_val, knn_feature_extractor.transform(X, is_train_data=False)], axis=1)\n",
    "    X = df_test[knn_features].fillna(0.0).values * knn_feature_weights\n",
    "    df_test_knn_features = knn_feature_extractor.transform(X, is_train_data=False)\n",
    "    for col in df_test_knn_features.columns:\n",
    "        df_test[col] = df_test_knn_features[col]\n",
    "    lgb_train = lgb.Dataset(\n",
    "        df_trn.loc[:, features],\n",
    "        label=target_trn,\n",
    "        feature_name=features,\n",
    "        categorical_feature=features_categorical\n",
    "    )\n",
    "    lgb_valid = lgb.Dataset(\n",
    "        df_val.loc[:, features],\n",
    "        label=target_val,\n",
    "        feature_name=features,\n",
    "        categorical_feature=features_categorical\n",
    "    )\n",
    "    lgb_params[\"learning_rate\"] = learning_rate + np.random.random() * 0.001  # おまじない\n",
    "    num_round = 999999999\n",
    "    model = lgb.train(\n",
    "        lgb_params,\n",
    "        lgb_train, \n",
    "        num_round, \n",
    "        valid_sets=[lgb_train, lgb_valid], \n",
    "        verbose_eval=300,\n",
    "        early_stopping_rounds=300 if num_round >= 1e8 else None,\n",
    "        fobj=None,\n",
    "        #feval=lgb_metric,\n",
    "    )\n",
    "    # cv\n",
    "    prediction_round = model.best_iteration+150 if num_round >= 1e8 else num_round  # おまじない\n",
    "    oof[indexes_val] = model.predict(df_val[features], num_iteration=prediction_round)\n",
    "    # feature importance\n",
    "    df_fold_importance = pd.DataFrame()\n",
    "    df_fold_importance[\"feature\"] = features\n",
    "    df_fold_importance[\"importance\"] = model.feature_importance()\n",
    "    df_fold_importance[\"fold\"] = fold_\n",
    "    df_feature_importance = pd.concat([df_feature_importance, df_fold_importance], axis=0)\n",
    "    # prediction for test data\n",
    "    predictions += model.predict(df_test[features], num_iteration=prediction_round) / N_SPLITS\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2663f92b-06ea-4190-b321-216b509b48ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score (not reliable!)\n",
      "  f1:  0.66850\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.59      0.69        32\n",
      "           1       0.56      0.43      0.48       205\n",
      "           2       0.70      0.60      0.65       191\n",
      "           3       0.82      0.77      0.79       362\n",
      "           4       0.71      0.60      0.65        45\n",
      "           5       0.62      0.50      0.56       126\n",
      "           6       0.69      0.40      0.51        50\n",
      "           7       0.64      0.61      0.63       334\n",
      "           8       0.72      0.79      0.76      1305\n",
      "           9       0.86      0.83      0.84        59\n",
      "          10       0.78      0.81      0.80      1337\n",
      "\n",
      "    accuracy                           0.74      4046\n",
      "   macro avg       0.72      0.63      0.67      4046\n",
      "weighted avg       0.73      0.74      0.73      4046\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = f1_score(target, oof.argmax(1), average=\"macro\")\n",
    "print(\"CV score (not reliable!)\")\n",
    "print(f\"  f1: {score:8.5f}\")\n",
    "print()\n",
    "print(classification_report(target, oof.argmax(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25cb96c-e854-413d-a82c-800ab5c64ad3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
